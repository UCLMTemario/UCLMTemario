<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Aika Kenshi" />
  <title>ARQUITECTURAS ESPECÍFICAS DE DOMINIO</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">ARQUITECTURAS ESPECÍFICAS DE DOMINIO</h1>
<p class="subtitle">TEMA 5</p>
<p class="author">Aika Kenshi</p>
<p class="date">15-06-2024</p>
</header>
<h1 id="introducción">Introducción</h1>
<ul>
<li><p>Desaparición de la ley de Moore y escala de Dennard.</p></li>
<li><p>Problemas:</p>
<ul>
<li><p>Consumo energético limitado: el rendimiento de incrementa,
reducienod severamente la energía por cada operación.</p></li>
<li><p>El acceso a memoria es un gran contribuyente para el consumo
energético.</p></li>
<li><p>Sobrecarga causada por el diseño arquitectual.</p></li>
</ul></li>
<li><p>Solución: arquitecturas específicas de dominio.</p></li>
<li><p>Caches no funcionan bien con patrones de acceso a memoria
predecibles, para procesamiento de vídeo sin reúso de datos y malgasto
de energía.</p></li>
<li><p>Solo necesita enfocarse en algunos kernels específicos.</p></li>
<li><p>DSAs usadas junto a CPUs-GPUs.</p></li>
<li><p>Lenguajes de descripción más apropiados.</p></li>
</ul>
<h1 id="directrives-para-dsa">Directrives para DSA</h1>
<ul>
<li><p>Uso de memorias dedicadas para minimizar movimiento de
datos.</p></li>
<li><p>Invierte recursos en más unidades aritméticas o memorias más
grandes.</p></li>
<li><p>Usa la forma de paralelismo más sencilla que coincide con el
dominio.</p></li>
<li><p>Reduce el tamaño de los datos y tipos al más simple que es
necesario para el dominio.</p></li>
<li><p>Usa un lenguaje de programación específico de dominio.</p></li>
</ul>
<h1 id="deep-neural-networks">Deep Neural Networks</h1>
<ul>
<li><p>Área concreta del campo de la inteligencia artificial,
relacionada con el machine learning y basada en redes
neuronales.</p></li>
<li><p>Inspirado en neuronas cerebrales.</p></li>
<li><p>Una red artificial realiza el cómputo de la salida en los pesos
basados en su entrada.</p></li>
<li><p>Un clster de redes neuronales es organizado en capas.</p></li>
<li><p>Cada capa busca distintas características.</p></li>
<li><p>En cada paso se genera unformación de más alto nivel.</p></li>
</ul>
<h2 id="entrenamiento">Entrenamiento</h2>
<ul>
<li><p>Cálculo de pesos.</p></li>
<li><p>Se realiza una vez.</p></li>
<li><p>Tareas de computación pesada.</p></li>
<li><p>Confía en GPUs.</p></li>
</ul>
<ol type="1">
<li><p>Aprendizaje supervisado: basado en un conjunto grande de
entrenamiento.</p></li>
<li><p>Aprendizaje no supervisado: basado en funciones de
incentivos.</p></li>
</ol>
<h2 id="inferencia">Inferencia</h2>
<ul>
<li><p>Después de que las DNNs hayan sido desplegadas en
producción.</p></li>
<li><p>Pasos de entrenamiento han sido completados y los pasos se han
definido.</p></li>
<li><p>Opimizar tarea recurrente mediante DNAs.</p></li>
</ul>
<h3 id="cnns">CNNs</h3>
<p>Redes neuronales convolucionales.</p>
<ul>
<li><p>Usado ampliamente en visión por computador.</p></li>
<li><p>Basado en la convolución y agrupación.</p></li>
<li><p>No son redes completamente conectadas.</p></li>
<li><p>Más simples y rápidas.</p></li>
</ul>
<h4 id="uso-de-lotes">Uso de lotes</h4>
<ul>
<li><p>Reusar pesos una vez capturados de la memoria a través de
múltiples entradas.</p></li>
<li><p>Incrementar la intensidad operacional.</p></li>
</ul>
<h4 id="cuantificación">Cuantificación</h4>
<p>Usa 8 o 16 bits de punto fijo.</p>
<h4 id="kernels">Kernels</h4>
<ul>
<li><p>Multiplicación de matriz-vector.</p></li>
<li><p>Multiplicación de matriz-matriz.</p></li>
<li><p>Stencii.</p></li>
<li><p>ReLU.</p></li>
<li><p>Sigmoide.</p></li>
<li><p>Tangente hiperbólica.</p></li>
</ul>
<h3 id="tensor-processing-unit">Tensor Processing Unit</h3>
<ul>
<li><p>DNN ASIC de Google.</p></li>
<li><p>Unidad de multiplicación de 8 bits 256x256.</p></li>
<li><p>Movimientos de memoria gestionados por software.</p></li>
<li><p>Co-procesador en el bus PCIe.</p></li>
</ul>
<h3 id="tpu-isa">TPU ISA</h3>
<ul>
<li><p>Read_Host_Memory: lee la memoria de CPU en el buffer
unificado.</p></li>
<li><p>Read_Weights: lee pesos de la memoria de pesos en la FIFO de
pesos como entrada de la unidad de la matriz.</p></li>
<li><p>MatrixMatrixMultiply/Convole: ejecuta una multiplicación
matriz-matriz, una multiplicación matricial por elementos, una
multiplicación vectorial por elementos o una convolución de los datos
del buffer unificado en los acumuladores.</p>
<ul>
<li>Toma una entrada de tamaño variable B<em>256, la multiplica por una
entrada constante de 256x256 y produce una salida B</em>256, tardando B
ciclos en cadena en completarse.</li>
</ul></li>
<li><p>Activate: ejecuta funciones de activación.</p></li>
<li><p>Write_Host_Memory: escribe datos del buffer unificado en la
memoria del host.</p></li>
<li><p>Usa memorias dedicadas: 24MiB de buffers dedicados, 4 MiB de
buffers acumuladores.</p></li>
<li><p>Invertir recursos en unidades aritméticas y memorias dedicadas:
60% de la memoria y 250X de las unidades aritméticas de una CPU
server-class.</p></li>
<li><p>Usar la forma de paralelismo más sencilla que coincida con el
dominio: explotar paralelismo SIMD 2D.</p></li>
<li><p>Usar un lenguaje de programación de dominio específico: usar
Tensor Flow.</p></li>
</ul>
<hr />
<p><a href="tag5_t1.html">TEMA 1: INTRODUCCIÓN</a></p>
<p><a href="tag5_t2.html">TEMA 2: PARALELISMO</a></p>
<p><a href="tag5_t3.html">TEMA 3: PROCESADORES SUPERESCALARES</a></p>
<p><a href="tag5_t4.html">TEMA 4: PARALELISMO A NIVEL DE HEBRAS</a></p>
<p><a href="tag5_t5.html">TEMA 5: ARQUITECTURAS ESPECÍFICAS DE
DOMINIO</a></p>
</body>
</html>
