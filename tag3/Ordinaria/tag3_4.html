<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Aika Kenshi" />
  <title>EXTRACCIÓN, TRANSFORMACIÓN Y CARGA DE DATOS</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">EXTRACCIÓN, TRANSFORMACIÓN Y CARGA DE DATOS</h1>
<p class="subtitle">TEMA 4</p>
<p class="author">Aika Kenshi</p>
<p class="date">17-11-2024</p>
</header>
<h1 id="integración-de-datos">1. INTEGRACIÓN DE DATOS</h1>
<p>Uno de los aspectos clave en inteligencia de negocio es proporcionar
información consistente, completa, limpia y actualizada que permita el
análisis y la toma de decisiones empresariales.</p>
<p>La <u>integración</u> combina datos de diferentes fuentes y los une
para, en última instancia, proporcionar una vista unificada.</p>
<p>Una gran parte del trabajo en un proyecto de Inteligencia de Negocio
consiste en diseñar y desarrollar procesos de integración de datos.</p>
<p>Se estima que unas <u>tres cuartas partes</u> del tiempo del proyecto
de Inteligencia de Negocio se dedican a la integración de datos.</p>
<p>Con tanto tiempo del proyecto dedicado a esta área, es fundamental
adoptar las mejores prácticas para diseñar procesos de integración de
datos robustos, escalables y que sean efectivos para la
organización.</p>
<p>Existen dos grandes paradigmas a la hora de diseñar e implementar
procesos ETLs: usar codigicación manual o utilizar herramientas y suites
comerciales.</p>
<p>Los procesos de integración de datos deben ser:</p>
<ul>
<li><p><u>Holísticos:</u> deben formar parte de los procesos de negocio
de la organización de manera integrada para evitar superposiciones e
incoherencias costosas.</p></li>
<li><p><u>Incrementales:</u> construir soluciones globales de
integración desde cero es un proceso costoso en tiempo y dinero, por lo
que es preferible implementar soluciones incrementales para hacer el
proceso más manejable y práctico.</p></li>
<li><p><u>Iterativos:</u> la solución adoptada necesita evolucionar
iterativamente para incorporar el aprendizaje adquirido con anterioridad
y adaptarse a las necesidades cambiantes del negocio.</p></li>
<li><p><u>Reutilizables:</u> aprovechar procesos realizados con
anterioridad para el ahorro de costes y ganar en consistencia en cuanto
a los datos.</p></li>
<li><p><u>Documentados:</u> la documentación permite identificar datos
que se han definido antes y posibilita la reutilización. Las
herramientas de integración de datos generan documentación de forma
automática, lo que facilita el trabajo del equipo de
integración.</p></li>
<li><p><u>Auditables:</u> las capacidades de auditoría permiten examinar
el linaje de los datos y realizar análisis de impacto, lo cual
proporciona una mejor comprensión del proceso de integración y facilita
el mantenimiento del mismo.</p></li>
</ul>
<p>El objetivo de los procesos de integración es conseguir las cinco C’s
de la información: clean (limpio), consistent (coherente), conformed
(conforme), current (actual) y comprehensive (completo).</p>
<h2 id="i.-proceso-manual-vs.-herramientas-etl">i. Proceso manual
vs. herramientas ETL</h2>
<p>El uso de herramientas de integración de datos, en particular las
herramientas de extracción, transformación y carga (ETL) son usadas
ampliamente en Inteligencia de Negocio de manera habitual, pero también
es frecuente que la extracción de los datos se realice mediante
codificaciones manuales.</p>
<p>Este tipo de extracciones de datos mediante procesos manuales suelen
ser más usadas por pequeñas y medianas empresas, pero también grandes
empresas que adoptaron herramientas de integración de datos para cargar
sus almacenes de datos empresariales suelen recurrir frecuentemente a
extracciones codificadas manualmente para alimentar sus almacenes de
datos de Inteligencia de Neogcio.</p>
<p>Las razones para usar la codificación manual en lugar de utilizar
herramientas son diversas, pero las herramientas proporcionan una serie
de <u>ventajas</u> que no dan procesos manuales.</p>
<h3 id="ventajas-de-usar-herramientas-etl">Ventajas de usar herramientas
ETL</h3>
<ul>
<li><p><u>Procesos dimensionales reutilizables:</u> las herramientas de
integración de datos ofrecen muchas funciones predefinidas o
transformaciones utilizadas en los procesos de integración de datos
basadas en buenas prácticas, que permiten su reutilización en sistemas
distintos.</p></li>
<li><p><u>Procesos robustos de calidad de datos:</u> muchas herramientas
de integración de datos incluyen procesos de limpieza, conformidad y
estandarización de datos que son críticos para la calidad de los datos,
pero que a menudo son demasiado complejos para implementarse utilizando
código SQL hecho a mano.</p></li>
<li><p><u>Workflow, control de errores y funcionalidad de
reinicio/recuperación:</u> las herramientas tienen la capacidad de
analizar el rendimiento para identificar cuellos de botella e
interacciones con otras aplicaciones. Con la codificación manual, todo
esto debe codificarse además del procesamiento básico de integración de
datos, incurriendo en un alto coste en tiempo y recursos.</p></li>
<li><p><u>Análisis de impacto y funcionalidad de dónde se utiliza
(linaje):</u> esto permite la gestión del cambio, mejora la
productividad y reduce los errores inesperados.</p></li>
<li><p><u>Autodocumentación de procesos y flujo de trabajo:</u> la
documentación (<strong>si es que existe</strong>) asociada con la
codificación manual es generalmente incompleta y
desactualizada.</p></li>
<li><p><u>Facilita la gestión de los datos:</u> las herramientas de
integración de datos que proporcionan repositorios y diccionarios
semánticos permiten la reutilización y el fácil mantenimiento de las
definiciones comerciales y técnicas, a menudo denominadas metadatos.
Esto permite que los proyectos de integración de datos posteriores
puedan aprovechar el trabajo existente acelerando, así, el tiempo de
desarrollo (time to market) y reduciendo los errores.</p></li>
<li><p><u>Mayor productividad:</u> con toda la funcionalidad ya
construida en las herramientas de integración de datos, ¿por qué una
organización de Tecnología de Información, incluso si tuviera el tiempo
y el dinero, tendría que volver a reinventarlo? Y no solo se consigue
ser más productivo por el hecho de que la funcionalidad ya está
integrada, sino porque las herramientas siguen las mejores prácticas de
la industria y de esta manera se evitan errores que fácilmente se
producirían sin ese conocimiento.</p></li>
</ul>
<h3
id="comparativa-de-costes-entre-el-uso-de-codificación-manual-y-herramientas-etl">Comparativa
de costes entre el uso de codificación manual y herramientas ETL</h3>
<p><img src="/img/2425/tag3/Tema4_1.png" /></p>
<h3
id="principales-herramientas-de-integración-de-datos-según-gartner-group">Principales
herramientas de integración de datos según Gartner Group</h3>
<ul>
<li><p><strong>Information PowerCenter:</strong> herramienta de
integración mejor valorada por Gartner. Conectores para todo tipo de
fuentes de datos. La integración de datos se realiza a modo punto a
punto con un modelo distribuido.</p></li>
<li><p><strong>Oracle GoldenGate, Oracle Data Integrator (ODI):</strong>
software destinado a la integración y recopilación de datos en tiempo
real en entornos de Tecnología de Información. ODI está destinado a
realizar procesos de integración de datos desde grandes volúmenes de
datos o grandes cargas de batch.</p></li>
<li><p><strong>IBM InfoSphere DataStorage:</strong> permite realizar
todo el proceso ETL completo en múltiples sistemas. Admite la
administración extendida de metadatos y la conectividad empresarial con
herramientas Big Data y Cloud.</p></li>
<li><p><strong>Microsoft SQL Server Integration Services
(SSIS):</strong> proporciona sus propios servicios de integración de SQL
Server para conectarlo a diferentes BBDD.</p></li>
<li><p><strong>Denodo Platform:</strong> acceso en tiempo real a los
datos integrados en las diversas fuentes de datos de una organización
(sin replica). Acceso a datos tanto estructurados como no estructurados
desde fuentes de datos propias de la organización como Big Data o
Cloud.</p></li>
<li><p><strong>Talend (Qlik) Open Studio:</strong> suite que aporta un
conjunto completo de herramientas de integración de datos mediante la
definición de workflows.</p></li>
</ul>
<h2 id="ii.-data-integration-services">ii. Data integration
services</h2>
<h1 id="proceso-de-extracción">2. PROCESO DE EXTRACCIÓN</h1>
<h2 id="i.-mapa-lógico-de-datos">i. Mapa lógico de datos</h2>
<h2 id="ii.-extraer-datos-de-fuentes-diversas">ii. Extraer datos de
fuentes diversas</h2>
<h2 id="iii.-buenas-prácticas-en-el-proceso-de-extracción">iii. Buenas
prácticas en el proceso de extracción</h2>
<h2 id="iv.-cambios-en-la-tabla-de-hechos">iv. Cambios en la tabla de
hechos</h2>
<h1 id="proceso-de-transformación">3. PROCESO DE TRANSFORMACIÓN</h1>
<h2 id="i.-calidad-y-limpieza-de-los-datos">i. Calidad y limpieza de los
datos</h2>
<h2 id="ii.-eliminación-de-redundancias">ii. Eliminación de
redundancias</h2>
<h2 id="iii.-homogeneización-de-datos">iii. Homogeneización de
datos</h2>
<h1 id="proceso-de-carga">4. PROCESO DE CARGA</h1>
<h2 id="i.-carga-de-dimensiones">i. Carga de dimensiones</h2>
<p>Cada vez que se crea un nuevo registro en una dimensión como
consecuencia de un proceso ETL de carga, se debe <u>crear un nuevo valor
de clave subrogada</u> que identifique de forma única al registro
insertado.</p>
<p>En un entorno de almacén de datos, las claves subrogadas para todas
las dimensiones podrían generarse a partir de una sola fuente.</p>
<p>En este caso, un elemento maestro de metadatos almacena la clave más
alta utilizada para todas las dimensiones simultáneamente.</p>
<p>Sin embargo, si hay muchos trabajos ETL concurrentes en ejecución,
podrían ser rivales por leer y escribir los valores almacenados con los
consiguientes problemas de acceso y actualzación de valores de
clave.</p>
<p>Por este motivo, se considera una buena práctica que se <u>establezca
un contador de clave subrogada para cada tabla de dimensiones por
separado.</u></p>
<p>Una posibilidad para formar la clave subrogada es concatenar la clave
natural del sistema de origen junto con un sello de tiempo que refleja
cuándo se creó el registro en el sistema de origen o cuándo se insertó
en el almacén de datos (intelligent or smart keys).</p>
<p>Vincular a la clave subrogada la hora exacta de su creación puede ser
útil en algunas situaciones, pero no es una alternativa aceptable por
las siguientes razones:</p>
<ul>
<li><p><u>Por definición:</u> una clave subrogada no debe tener valor
para el sistema de inteligencia de negocio más allá de identificar
unívocamente cada registro. Si se vincula a la clave del sistema de
origen y esta cambia, la clave deberá actualizarse en todo el sistema
para mantener esa relación con la fuente de datos de origen.</p></li>
<li><p><u>Por rendimiento:</u> concatenar la clave del sistema de origen
con un sello de fecha degrada significativamente el rendimiento de la
consulta. Al no haber control sobre las claves del sistema de origen, es
muy probable tener que utilizar los tipos de datos CHAR o VARCHAR para
las claves subrogadas. Además, al agregar el sello de tiempo, se le
agregan 16 carácteres o más, por lo que el atributo puede volverse
difícil de manejar. Esta clave debe propagarse por las tablas de hechos
(normalmente con millones de registros) por lo que el espacio para
almacenar los datos y los índices sería excesivo y provocaría una
notable caída del rendimiento tanto en los procesos ETL como en las
consultas.</p></li>
<li><p><u>Por la existencia de fuentes heterogéneas:</u> la
concentración de la clave natural y del sello de fecha solo puede
llevarse a cabo en un entorno homogeneo. Lo habitual en sistemas de
inteligencia de negocio es que los datos de las dimensiones provengan de
muchas fuentes de datos diferentes. Cada uno de estos sistemas fuente
tiene su propio propósito y puede identificar los mismos valores de una
dimensión de manera diferente, por lo que el uso de este tipo de claves
es prácticamente imposible.</p></li>
</ul>
<p>En la mayoría de los casos, se considera una buena práctica usar un
<u>valor numérico</u> para la clave subrogada en la dimensión por las
razones de rendimiento y de gestión de la información.</p>
<h3 id="dimensiones-de-tiempo-y-fecha">Dimensiones de tiempo y
fecha</h3>
<p>Cada dimensión de fecha necesita como mínimo <u>un atributo de tipo
de fecha</u> y <u>un atributo que contenga la clave</u> de la
dimensión.</p>
<p>Se considera una buena práctica incluir en la dimensión un registro
que refleje un <u>valor especial</u> para aquellas situaciones de fecha
en la que la fecha registrada no es aplicable, está corrupta o no ha
sucedido todavía.</p>
<p>La clave subrogada para la dimensión de fecha es la única que no
sigue la recomendación de que sea un valor numérico sin sentido para el
negocio.</p>
<p>Se considera una buena práctica usar un valor numérico siguiendo el
formato YYYYMMDD, lo cual permite identificar el valor representado en
el registro.</p>
<p>En el caso del registro para mostrar el valor especial (fecha que por
alguna razón no aplica en la tabla de hechos) se puede usar el valor
<strong>99999999</strong>.</p>
<h3 id="slowly-changing-dimensions---scd-type-1">Slowly Changing
Dimensions - SCD Type 1</h3>
<p>El proceso ETL optará por el tipo 1 si los datos que se están
modificando es debido a una corrección o porque desde la organización no
hay interés en mantener el historial de los valores.</p>
<p>El SCD Tipo 1 nunca afectará a la clave de la dimensión
modificada.</p>
<p>La técnica de implementación más fácil es usar sentencias SQL UPDATE
para la actualización de los atributos de la dimensión.</p>
<p>Desafortunadamente, SQL UPDATE es una transacción de <u>bajo
rendimiento</u> y puede aumentar considerablemente la ejecución del
proceso ETL.</p>
<p>Para cambios muy grandes de Tipo 1, la mejor manera de reducir el
tiempo de ejecución es emplear una tabla intermedia de carga masiva
(bulk loader) en la que se insertarán los registros modificados para,
posteriormente, eliminar los registros afectados de la tabla de
dimensión e insertar los nuevos de la tabla intermedia.</p>
<h3 id="slowly-changing-dimensions---scd-type-2">Slowly Changing
Dimensions - SCD Type 2</h3>
<p>El SCD tipo 2 requiere un buen sistema CDC.</p>
<p>Los cambios en los datos de origen deben detectarse tan pronto como
ocurran para que, de este modo, el registro afectado en el almacén de
datos pueda ser actualizado.</p>
<p>Si el sistema fuente no notifica al almacén de datos los cambios y no
persiste la fecha de sus propias actualizaciones, el almacén de datos se
ve obligado a descargar la dimensión completa y buscar registro por
registro y campo por campo para detectar los cambios que han ocurrido
desde la última vez que se descargó la dimensión de la fuente (técnica
de comparación de registros CDC).</p>
<p>Se considera una buena práctica agregar los siguientes cinco campos a
las tablas de dimensiones procesadas:</p>
<ol type="1">
<li><p>Fecha de cambio (FK): permite a un usuario final utilizar el
calendario laboral (días festivos, días de pago y periódos fiscales)
para analizar cuántos cambios de un determinado tipo se hicieron en
ciertos periodos importantes para el negocio.</p></li>
<li><p>DataTime de inicio (fecha y hora exactas del cambio).</p></li>
<li><p>DataTime de fin (fecha y hora exactas de próximo cambio): las dos
marcas de fecha y hora permiten definir un intervalo de relevancia para
que las consultas puedan obtener registros válidos usando la sentencia
SQL BETWEEN.</p></li>
<li><p>Motivo del cambio (campo de texto): este dato deberá venir del
proceso ETL que dio lugar al registro modificado en la
dimensión.</p></li>
<li><p>Flag actual (current/expired).</p></li>
</ol>
<h3 id="slowly-changing-dimensions">Slowly Changing Dimensions</h3>
<p>Si el sistema fuente no notifica los cambios y no marca con fecha las
actualizaciones, el almacén de datos se ve obligado a descargar la tabla
completa y buscar registro a registro los cambios desde la última vez
que se actualizó la dimensión.</p>
<p>En este caso, es preciso extraer previamente toda la información para
realizar dicha búsqueda, persistiendo los registros modificados en un
almacenamiento intermedio (dimensión maestra de referencia cruzada) para
que la información se organice adecuadamente en el proceso ETL.</p>
<h2 id="ii.-carga-de-hechos">ii. Carga de hechos</h2>
<p>Las tablas de hechos están definidas por la granularidad (nivel de
detalle) de las medidas del negocio que queremos representar en
ellas.</p>
<p>Se considera una buena práctica definir el nivel de detalle más alto
de la actividad del engocio para que el análisis sea el más completo
posible.</p>
<p>Las tablas de hechos poseen un conjunto de claves foráneas conectadas
a las dimensiones que proporcionan el contexto de las mediciones del
negocio, así como uno o más campos numéricos de medida (hechos) y otros
campos adicionales que completan la estructura de la tabla.</p>
<p>La tabla de hechos deberá tener una clave que identifique
unívocamente cada registro.</p>
<p>Si no es posible hacerlo con el conjunto de claves ajenas a las
dimensiones, lo mejor es usar una clave subrogada de tipo numérico
(aunque no tenga valor para el negocio) o bien usar una degenerate
dimension incluyendo como clave un atributo NK del Sistema de
Registro.</p>
<p>En el modelado dimensional, es fundamental mantener la <u>integridad
referencial de la tabla de hechos con las dimensiones,</u> es decir, que
no haya registros de la tabla de hechos que contengan referencias de
clave ajena corruptas o desconocidas.</p>
<p>Hay dos formas de violar la integridad referencial en una
dimensión:</p>
<ul>
<li><p>Cargar un registro de hechos con una o más claves foráneas
incorrectas.</p></li>
<li><p>Eliminar un registro de dimensión cuya clave principal se esté
utilizando en la tabla de hechos.</p></li>
</ul>
<p>Un hecho que viola la integridad referencial es un problema
importante para el sistema de Inteligencia de Negocio.</p>
<p>Aparentemente, el registro tiene cierta legitimidad, ya que
probablemente representa una medida real del negocio, pero cualquier
análisis que haga referencia a la dimensión incorrecta devolverá
información no veraz (y, por tanto, sin valor para la organización).</p>
<p>Los tres lugares principales en el proceso ETL donde se puede hacer
cumplir la integridad referencial son:</p>
<ol type="1">
<li><p><u>Antes</u> de cargar el hecho, verificando que los registros
contienen los valores de clave correctas contra la dimensión.</p></li>
<li><p>Cumplimiento de la integridad referencial en la propia base de
datos <u>en el momento</u> de cada inserción y eliminación de la tabla
de hechos.</p></li>
<li><p>Una vez cargados los registros, <u>escaneando</u> regularmente la
tabla de hechos, buscando datos incorrectos de claves ajenas.</p></li>
</ol>
<p>Todas las tablas de dimensiones deben ser actualizadas ante nuevas
inserciones de entidades y/o cambios de Tipo 2 <u>antes de cargar los
datos en las tablas de hechos.</u></p>
<p>El proceso es a la inversa al <u>eliminar registros,</u> primero se
eliminan los registros de las tablas de hechos no deseados y
posteriormente se hace lo propio con los registros de dimensiones al no
tener ya ningún vínculo con la tabla de hechos.</p>
<p>Al construir o cargar una tabla de hechos, el paso final del proceso
ETL es <u>convertir las claves naturales de los nuevos registros en las
claves subrogadas actuales</u> para cada entidad de dimensión.</p>
<p>Consideraciones a la hora de cargar los registros en las tablas de
hechos (<strong>nuevas inserciones y actualizaciones</strong>):</p>
<ul>
<li><p><u>Separar las inserciones de las actualizaciones:</u> aunque
muchas herramientas ETL ofrecen la funcionalidad UPDATE ELSE INSERT, se
trata de un proceso lento por lo que es conveniente que dichos procesos
puedan separar ambas operaciones. La mejor práctica es <strong>aplicar
primero las actualizaciones y posteriormente cargar de forma masiva los
nuevos registros</strong> para obtener un rendimiento óptimo del proceso
de carga.</p></li>
<li><p><u>Utilizar una herramienta de carga masiva (bulk-load):</u> usar
una herramienta de carga masiva en lugar de sentencias SQL INSERT para
cargar datos mejora drásticamente el rendimiento de la carga. Los
principales fabricantes de base de datos proporcionan este tipo de
herramientas.</p></li>
<li><p><u>Carga en paralelo:</u> el proceso de carga se optimiza si se
separan físicamente en segmentos.</p></li>
<li><p><u>:</u> .</p></li>
</ul>
</body>
</html>
