<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Aika Kenshi" />
  <title>ANÁLISIS DE DATOS</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">ANÁLISIS DE DATOS</h1>
<p class="subtitle">TEMA 5</p>
<p class="author">Aika Kenshi</p>
<p class="date">09-01-2025</p>
</header>
<h1 id="conceptos-básicos-del-análisis-de-datos">CONCEPTOS BÁSICOS DEL
ANÁLISIS DE DATOS</h1>
<h2 id="i.-definiciones">i. Definiciones</h2>
<p>El análisis de datos es la ciencia que se encarga de examinar un
conjunto de datos con el propósito de obtener tendencias, patrones de
comportamiento o conclusiones generales sobre la información para
mejorar la toma de decisiones, o simplemente para ampliar los
conocimientos sobre diversos temas.</p>
<p>Existen dos tipos de análisis de datos desde el punto de vista de
<strong>la naturaleza de la información a analizar</strong>:</p>
<ul>
<li><p><u>Cuantitativo:</u> la información es numérica a partir de la
cual se pueden elaborar estadísticas exactas.</p></li>
<li><p><u>Cualitativo:</u> se trata de información obtenida de una base
de datos presentada usualmente en formato texto.</p></li>
</ul>
<h2 id="ii.-tipos-de-análisis-de-datos">ii. Tipos de análisis de
datos</h2>
<p>En base al <strong>tipo de conocimiento</strong> que deseamos extraer
de la información, podemos identificar cinco tipos de análisis de
datos:</p>
<ul>
<li><p><u>Análisis descriptivo:</u> determina qué ha pasado.</p></li>
<li><p><u>Análisis de diagnóstico:</u> determina por qué ha
pasado.</p></li>
<li><p><u>Análisis predictivo:</u> permite descubrir qué puede ocurrir
en el futuro.</p></li>
<li><p><u>Análisis prescriptivo:</u> ayuda a seleccionar las mejores
estrategias para conseguir un objetivo concreto.</p></li>
<li><p><u>Análisis cognitivo:</u> combina una serie de tecnologías
avanzadas como el <i>deep learning</i>, algoritmos de aprendizaje
automático (<i>machine learning</i>) e inteligencia artificial (IA) para
simular el comportamiento del cerebro humano y proporcionar resultados
similares a cómo piensan las personas.</p></li>
</ul>
<h3 id="análisis-descriptivo">Análisis descriptivo</h3>
<p>El análisis descriptivo responde a preguntas sobre <u>lo que ha
sucedido</u>, en función de los datos históricos, para fundamentar las
decisiones sobre el futuro.</p>
<p>Las técnicas de análisis descriptivo resumen grandes conjuntos de
datos para presentar información a las partes interesadas.</p>
<p>El descriptivo es el tipo de análisis más común y suele realizarlo
los analistas de datos.</p>
<p>El desarrollo de indicadores de rendimiento (KPI) y de otras medidas
relativas ayuda a realizar un seguimiento del éxito o el fracaso de los
objetivos empresariales.</p>
<p>La presentación de los datos en relación con esos KPI es un análisis
descriptivo.</p>
<p>Normalmente el análisis descriptivo parte de un <u>gran conjunto de
datos</u> que a simple vista no ofrecen mucha información, pero que, al
utilizar aplicaciones de software para su tratamiento (limpieza,
ordenamiento, transformación, visualización), permiten expresar de una
forma comprensible lo que ha venido pasando en las organizaciones.</p>
<p>Con esta información se toman acciones que se adapten a los objetivos
de la empresa.</p>
<p>El análisis descriptivo de datos es el tipo de análisis que está
presente en la gran mayoría de organizaciones y es por el que
normalmente se comienza.</p>
<p>En este tipo de análisis es común observar tableros de control,
gráficas de barras, gráficas de pasteles, infografías y otros.</p>
<h3 id="análisis-de-diagnóstico">Análisis de diagnóstico</h3>
<p>El análisis de diagnóstico ayuda a responder preguntas sobre las
causas de las situaciones acontecidas y suele ser el paso siguiente en
el análisis de datos (después del análisis descriptivo).</p>
<p>Los analistas toman los resultados del análisis descriptivo y
profundizan en la búsqueda de la causa, es decir, <u>por qué ha
ocurrido</u>.</p>
<p>Las métricas y los indicadores de interés se investigan aún más para
descubrir por qué han sido mejores o peores.</p>
<p>El análisis de diagnóstico lo suelen realizar los analistas de datos
y los científicos de datos, y suele hacerse en tres pasos:</p>
<ol type="1">
<li><p>Identificación de anomalías en los datos. Las anomalías pueden
ser cambios inesperados en una métrica o en un mercado
determinado.</p></li>
<li><p>Recopilación de datos relacionados con estas anomalías.</p></li>
<li><p>Uso de técnicas estadísticas para detectar relaciones y
tendencias que expliquen estas anomalías.</p></li>
</ol>
<h3 id="análisis-predictivo">Análisis predictivo</h3>
<p>El análisis predictivo consiste en la aplicación de técnicas y
modelos matemáticos y estadísticos sobre los datos históricos que posee
la organización para identificar las tendencias y determinar si es
posible que se repitan, lo cual ayuda a responder a preguntas sobre
<u>lo que ocurrirá en el futuro</u>.</p>
<p>Engloba técnicas estadísticas y de aprendizaje automático (<i>machine
learning</i>) como las de previsión, redes neuronales, árboles de
decisión y regresión.</p>
<p>El análisis predictivo suelen realizarlo los analistas de datos y los
científicos de datos.</p>
<h3 id="análisis-prescriptivo">Análisis prescriptivo</h3>
<p>El análisis prescriptivo va un paso más allá del análisis predictivo
y <u>ayuda a responder preguntas sobre las acciones que deben llevarse a
cabo para lograr un objetivo</u>.</p>
<p>Esta técnica permite que, en caso de incertidumbre, las empresas
tomen decisiones basadas en datos.</p>
<p>Las técnicas de análisis prescriptivo dependen de estrategias de
aprendizaje automático para buscar patrones en conjuntos de datos de
gran tamaño.</p>
<p>Mediante el análisis de eventos y decisiones anteriores, se puede
calcular la probabilidad de otros resultados.</p>
<p>El análisis prescriptivo suelen realizarlo los científicos de
datos.</p>
<h3 id="análisis-cognitivo">Análisis cognitivo</h3>
<p>La analítica cognitiva tiene como propósito diseñar modelos que
mediante una combinación de algoritmos de <i>machine learning</i>,
<i>deep learning</i> e <i>IA</i> sean capaces de entender, apreder,
reconocer y reproducir el funcionamiento de la mente humana.</p>
<p>El desarrollo de esta área supone un cambio en el paradigma de la
relación entre las personas y los ordenadores, pues dejan de ser meros
instrumentos al servicio de las personas para trabajar codo con codo
formando así una sinergia humano-máquina.</p>
<p>La computación cognitiva extiende la capacidad de los sistemas de
computación a una nueva dimensión, en la que el razonamiento y las
emociones humanas son entendidas y aprendidas por las máquinas para
hacer visible lo relevante dentro de la inmensidad que proporciona el
actual universo de datos.</p>
<h2 id="iii.-proceso-de-análisis-de-datos">iii. Proceso de análisis de
datos</h2>
<p>El desarrollo de una solución de análisis comienza, antes de implicar
cualquier tecnología, con un ejercicio de recopilación de
requisitos.</p>
<p>A partir de ahí, el proceso continúa con la ingesta, el procesamiento
y la exploración de los datos.</p>
<p>Tras el análisis y la implementación de soluciones, se requiere del
<i>feedback</i> de los usuarios del negocio para determinar la idoneidad
de la solución de análisis implementada y poder optimizarla.</p>
<h3 id="recopilación-de-requisitos">Recopilación de requisitos</h3>
<p>Los equipos de datos trabajan con la organización para entender las
necesidades empresariales y los resultados previos de un proyecto de
análisis.</p>
<p>La recopilación de requisitos incluye la identificación de los
aspectos siguientes:</p>
<ul>
<li><p>¿Cuáles son las preguntas empresariales clave?</p></li>
<li><p>¿Cuáles son los datos disponibles? ¿Responderán los datos
disponibles a las necesidades empresariales o es necesario recopilar más
datos?</p></li>
<li><p>¿Cuáles son las dimensiones esenciales? ¿Cómo querrán las partes
interesadas segmentar y cortar los datos?</p></li>
<li><p>¿Cuáles son los indicadores clave de rendimiento o las métricas
de rendimiento?</p></li>
<li><p>¿Cómo consumirán los usuarios el análisis?</p></li>
<li><p>¿Cuál es la frecuencia de ingesta de datos?</p></li>
<li><p>¿Cuál es la frecuencia de creación de informes?</p></li>
</ul>
<p>Es un error frecuente pensar que un equipo de datos pueda extraer
información de volúmenes de datos sin haber analizado ninguna de las
preguntas anteriores.</p>
<p>Un equipo de datos no podrá determinar el tipo de análisis adecuado
ni la solucón correcta sin haber seguido un proceso estructurado de
recopilación de requisitos.</p>
<h3 id="ingesta-y-procesamiento-de-datos">Ingesta y procesamiento de
datos</h3>
<p>Este proceso consiste en identificar las fuentes y los modelos
lógicos de datos, y diseñar los procesos ETL (o ELT) correspondientes
que permitan alimentar el almacén de datos donde se realizará el
análisis de la información.</p>
<h3 id="exploración-de-datos">Exploración de datos</h3>
<p>La exploración de datos es el esfuerzo por comprender con qué tipo de
información se está trabajando y cómo esos datos pueden responder a las
necesidades de la empresa.</p>
<p>La exploración de datos se puede realizar con muchas herramientas
distintas, y cada miembro del equipo de datos puede generar perfiles de
datos en una herramienta diferente.</p>
<p>La exploración de datos ayuda a informarse sobre los pasos necesarios
de limpieza y transformación de los datos, lo cual puede comunicarse a
su vez al ingeniero de datos para incorporarlo en la solución del
análisis.</p>
<p>El analista también puede comenzar la creación de prototipos de
paneles o informes en la fase de exploración de los datos.</p>
<p>Entender la forma en que la empresa quiere ver y usar los resultados
del análisis ayudará a dar forma al prototipo.</p>
<h3 id="análisis-de-datos-e-implementación-de-una-solución">Análisis de
datos e implementación de una solución</h3>
<p>Los resultados del análisis deben responder a las necesidades
empresariales identificadas y, tras la revisión inicial, es probable que
den lugar a más preguntas y análisis.</p>
<p>Dichos resultados se presentarán a las partes interesadas en una
herramienta de visualización de datos o de creación de informes donde
los usuarios del negocio pueden iteractuar con los resultados y usarlos
para la toma de decisiones.</p>
<h3 id="feedback-del-proceso-y-optimización">Feedback del proceso y
optimización</h3>
<p>La implementación de una solución de análisis parece el fin del
proceso, pero es importante entender las respuestas a algunas preguntas
clave:</p>
<ul>
<li><p>¿Responde el análisis realmente a las necesidades
empresariales?</p></li>
<li><p>¿Existen problemas técnicos imprevistos con la solución?</p></li>
<li><p>¿Qué nuevas preguntas empresariales plantea este
análisis?</p></li>
</ul>
<p>Si el producto implicado <u>no responde adecuadamente a sus
necesidades</u>, es evidente que el proceso seguido no ha sido el
adecuado.</p>
<p>Con el <i>feedback</i> de los usuarios sobre el sistema de análisis
implantado, se lleva a cabo la optimización del sistema para que se
ajuste de manera más precisa a las necesidades del negocio.</p>
<h1 id="análisis-de-datos-con-olap">2. ANÁLISIS DE DATOS CON OLAP</h1>
<h2 id="i.-conceptos-olap">i. Conceptos OLAP</h2>
<p>El procesamiento analítico en línea (<i>OnLine Analitical
Processing</i> - OLAP acuñado por Edgar F. Codd) es una tecnología de
análisis de datos que permite extraer conocimiento de datos (normalmente
empresariales) desde diferentes puntos de vista.</p>
<p>Los sistemas OLAP almacenan datos multidimensionales al representar
la información en más de dos dimensiones o categorías.</p>
<p>Las estructuras bidimensionales (presentación tabular tradicional)
muestran los datos en columnas y filas, mientras que las
multidimensionales tienen varias características o dimensiones que
permiten contextualizar la información.</p>
<p>El análisis OLAP es un tipo de análisis <u>cuantitativo</u>, es
decir, basado en el cálculo numérico de datos.</p>
<p>Por ello, las herramientas OLAP están diseñadas específicamente para
aquellos sectores de negocio que basan su estrategia en la evolución y
comportamiento de datos numéricos a través del tiempo (típicamente en
las áreas de contabilidad y finanzas).</p>
<p>La potencia del análisis OLAP radica en la posibilidad de realizar
cálculos de manera ágil pudiendo mostrar la información desde diferentes
puntos de vista.</p>
<h3 id="arquitectura-olap">Arquitectura OLAP</h3>
<p>Los sistemas OLAP son diferentes a otras formas de conceptualización
de datos porque maneja los datos de la misma manera que lo hacen las
personas cuando están creando informes.</p>
<p>Estos sistemas están diseñados para trabajar en conjunto con las
herramientas pertenecientes a la arquitectura de inteligencia de
negocio, es decir, procesos ETL y el almacén de datos.</p>
<p>El sistema OLAP típicamente consta de dos categorías distintas de
software:</p>
<ul>
<li><p>El cubo OLAP que alberga los datos multidimensionales.</p></li>
<li><p>Las herramientas de acceso que permiten a los usuarios manipular
la información de forma interactiva para realizar el análisis de
negocio.</p></li>
</ul>
<h3 id="cubo-olap">Cubo OLAP</h3>
<p>Un <u>cubo de datos OLAP o Hipercubo</u> es una estructura de datos
diseñada especialmente para el análisis de grandes cantidades de
información.</p>
<p>Si bien es más fácil visualizarlo como un modelo de datos
tridimensional, la mayoría de los cubos de datos tienen más de tres
dimensiones.</p>
<p>Los cubos OLAP son rígidos, ya que no se pueden cambiar las
dimensiones ni los datos subyaacentes una vez creados.</p>
<p>En el lenguaje OLAP, una dimensión se refiere a una característica o
perspectiva de los datos, no a una dirección del espacio.</p>
<p>Las dimensiones permiten mostrar medidas de negocio desde diferentes
puntos de vista, sumarizando o detallando de manera ágil la información
para descubrir patrones o tendencias en los datos.</p>
<h2 id="ii.-tipos-de-implementación">ii. Tipos de implementación</h2>
<h3 id="rolap-relational-olap">ROLAP (Relational OLAP)</h3>
<p>Este tipo de organización física se implementa sobre tecnología
relacional, pero agregándole extensiones y herramientas para poder
utilizarlo como un analizador OLAP.</p>
<p>ROLAP emula el cubo insertando una capa semántica entre la base de
datos relacional y la herramienta del usuario final que imita las
acciones del cubo de datos.</p>
<p>En los sistemas ROLAP, los cubos multidimensionales se generan
dinámicamente al instante de realizar las diferentes consultas, haciendo
de esta manera el manejo de cubos transparente a los usuarios.</p>
<p>Para emular el modelo dimensional de los sistemas OLAP, las
implementaciones ROLAP utilizan los diferentes esquemas ya conocidos (en
estrella, copo de nieve y constelación) los cuales transformarán el
modelo multidimensional y permitirán que pueda ser gestionado por un
SGBD Relacional ya que solo se almacenarán tablas.</p>
<p>El uso de ROLAP permite a las empresas poder utilizar las bases de
datos relacionales ya implantadas en la organización, lo cual es un
valor añadido a la hora de inclinarse por una implementación de este
tipo.</p>
<p>Las principales desventajas de esta implementación son las
siguientes:</p>
<ul>
<li><p>Las bases de datos relacionales no están diseñadas para
estructurar los datos en un formato dimensional, por lo que cuando la
información adquiere más dimensiones y mayor complejidad jerárquica el
rendimiento de este tipo de sistemas se ve gravemente afectado. Pese a
todo, las bases de datos relacionales actuales poseen altas capacidades
para la gestión de grandes volúmenes de datos, por lo que las
implementaciones ROLAP son perfectamente válidas para el análisis de
información.</p></li>
<li><p>Los datos de los cubos se deben calcular cada vez que se ejecuta
una consulta sobre ellos. Esto provoca que ROLAP no sea muy eficiente en
cuanto a la rapidez de respuesta ante las consultas de los usuarios (es
posible incrementar la velocidad de respuesta almacenando los resultados
obtenidos de ciertas consultas en la memoria caché).</p></li>
</ul>
<h3 id="molap-multidimensional-olap">MOLAP (multidimensional OLAP)</h3>
<p>El objetivo de los sistemas MOLAP es almacenar físicamente los datos
en estructuras dimensionales de manera que la representación externa y
la interna coincidan.</p>
<p>Para ello, se dispone de estructuras de almacenamiento específicas
(<i>arrays</i>) y técnicas de compactación de datos que favorecen el
rendimiento del sistema.</p>
<p>Las ventajas de la implementación MOLAP son:</p>
<ul>
<li><p>Consultas rápidas debido a la optimización del rendimiento de
almacenamiento, la indexación multidimensional y la memoria
caché.</p></li>
<li><p>Automatización del procesamiento de los datos agregados de mayor
nivel.</p></li>
<li><p>El modelo de almacenamiento en vectores/matrices proporciona una
indexación natural.</p></li>
<li><p>Eficaz extracción de datos lograda gracias a la
pre-estructuración de los datos agregados.</p></li>
</ul>
<p>Por el contrario, la implementación MOLAP también tiene algunas
desventajas:</p>
<ul>
<li><p>Si es preciso realizar cambios sobre algún cubo, hay que
generarlo totalmente para que se reflejen las modificaciones llevadas a
cabo.</p></li>
<li><p>Se precisa más espacio físico para almacenar dichos datos (poco
significativo).</p></li>
<li><p>La fase de carga de datos puede ser bastante larga, sobre todo
para grandes volúmenes de datos. Normalmente, esto se puede evitar con
un procesamiento incremental, es decir, solo el procesamiento de los
datos que han cambiado (por lo general, los nuevos datos) en lugar de
volver a procesar todo el conjunto de datos.</p></li>
<li><p>Algunas herramientas MOLAP tienen dificultades para actualizar y
consultar los modelos de más de diez dimensiones. Este límite varía en
función de la complejidad y cardinalidad de las dimensiones manejadas,
así como de la cantidad de hechos o medidas almacenadas.</p></li>
</ul>
<h3 id="holap-hybrid-olap">HOLAP (Hybrid OLAP)</h3>
<p>Constituye un sistema híbrido entre MOLAP y ROLAP combinando estas
dos implementaciones para almacenar algunos datos en un motor relacional
y otros en una base de datos multidimensional.</p>
<p>Los datos agregados y precalculados se almacenan en estructuras
multidimensionales, mientras que los de menor nivel de detalle se
persisten en estructuras relacionales.</p>
<p>Con esta implementación, se utilizará ROLAP para navegar y explorar
los datos y se empleará MOLAP para el proceso de análisis (realización
de los <i>dashboards</i>).</p>
<h2 id="iii.-operaciones-olap">iii. Operaciones OLAP</h2>
<p>Una consulta de un almacén de datos consiste en la obtención de
indicadores a partir de los datos de una tabla de hechos, restringidas
por las propiedades o condiciones de los atributos y contextualizadas
por las dimensiones modeladas en el sistema.</p>
<p>Las operaciones que se pueden realizar sobre modelos
multidimensionales y que son las que verdaderamente les permitirán a los
usuarios explorar e invertigar los datos en busca de respuestas,
son:</p>
<ul>
<li><p>Drill-down.</p></li>
<li><p>Drill-up.</p></li>
<li><p>Drill-across.</p></li>
<li><p>Roll-across.</p></li>
<li><p>Pivot.</p></li>
<li><p>Page (Slice).</p></li>
<li><p>Drill-though.</p></li>
<li><p>Dice.</p></li>
</ul>
<h3 id="drill-down">Drill-down</h3>
<p>Permite apreciar los datos en un mayor detalle, bajando por una
jerarquía definida en un cubo.</p>
<p>Esto brinda la posibilidad de introducir un nuevo nivel o criterio de
agregación en el análisis, disgregando los grupos actuales.</p>
<p>Drill-down es ir <strong><u>de lo general a lo
específico</u></strong>.</p>
<p><img src="/img/2425/tag3/Tema5_1.png" /></p>
<h3 id="drill-up">Drill-up</h3>
<p>Permite apreciar los datos en menor nivel de detalle, subiendo por
una jerarquía definida en un cubo.</p>
<p>Esto brinda la posibilidad de quitar un nivel o criterio de
agregación en el análisis, agregando los grupos actuales.</p>
<p>Drill-up es ir <strong><u>de lo específico a lo
general</u></strong>.</p>
<p><img src="/img/2425/tag3/Tema5_2.png" /></p>
<h3 id="drill-across">Drill-across</h3>
<p>Funciona de forma similar a drill-down, con la diferencia de que
drill-across no se realiza sobre una jerarquía, sino que su forma de ir
<strong><u>de lo general a lo específico es agregar un
atributo</u></strong> a la consulta como nuevo criterio de análisis.</p>
<p><img src="/img/2425/tag3/Tema5_3.png" /></p>
<h3 id="roll-across">Roll-across</h3>
<p>Funciona de forma similar a drill-up, con la diferencia de que
roll-across no se hace sobre una jerarquía, sino que su forma de ir
<strong>de lo específico a lo general es quitar el atributo</strong> de
la consulta, eliminando de esta manera un criterio de análisis.</p>
<p><img src="/img/2425/tag3/Tema5_4.png" /></p>
<h3 id="pivot">Pivot</h3>
<p>Permite seleccionar el orden de visualización de los atributos e
indicadores, con el objetivo de analizar la información desde diferentes
perspectivas.</p>
<p><img src="/img/2425/tag3/Tema5_5.png" /></p>
<h3 id="page-slice">Page (Slice)</h3>
<p>Presenta el cubo dividido en secciones a través de los valores de un
atributo, como si se tratase de páginas de un libro.</p>
<p><img src="/img/2425/tag3/Tema5_6A.png" /> <strong>Página 1 - Producto
1</strong></p>
<p><img src="/img/2425/tag3/Tema5_6B.png" /> <strong>Página 2 - Producto
2</strong></p>
<h3 id="drill-though">Drill-though</h3>
<p>Permite apreciar los datos en su máximo nivel de detalle.</p>
<p>Esto brinda la posibilidad de analizar cuáles son los datos
relacionados al valor de un Indicador, que se ha sumarizado dentro del
cubo multidimensional.</p>
<p><img src="/img/2425/tag3/Tema5_7.png" /></p>
<h3 id="dice">Dice</h3>
<p>Permite extraer un subcubo del cubo dimensional analizado para
enfatizar o remarcar los datos de dos o más dimensiones.</p>
<p><img src="/img/2425/tag3/Tema5_8.png" /></p>
<h1 id="data-mining-en-análisis-empresarial">3. DATA MINING EN ANÁLISIS
EMPRESARIAL</h1>
<h2 id="i.-conceptos-básicos-de-minería-de-datos">i. Conceptos básicos
de minería de datos</h2>
<p><strong><i>Data mining</i></strong> (minería de datos) es un conjunto
de técnicas y tecnologías que ayudan a descubrir patrones de
comportamiento que se repiten de manera consistente entre grandes
cantidades de datos.</p>
<p>Es un proceso que ayuda a <u>segmentar</u> importantes volúmenes de
datos y transformarlos en información útil para el usuario.</p>
<p>Forma parte de un proceso global de identificación de patrones
potencialmente útiles y entendibles denominado KDD (<i>Knowledge
Discovery in Databases</i>), cuyo objetivo es el tratamiento de fuentes
de datos masivas para la extracción de conocimiento.</p>
<p>La capacidad predictiva de <i>data mining</i> ha cambiado el diseño
de las estrategias empresariales.</p>
<p>Estos son algunas aplicaciones de <i>data mining</i> en la industria
actual:</p>
<h3 id="marketing">Marketing</h3>
<p>Analizando las relaciones entre parámetros es posible adivinar su
comportamiento para dirigir campañas personalizadas de fidelización o
captación.</p>
<p><i>Data mining</i> en marketing predice también qué usuarios pueden
darse de baja de un servicios, qué les interesa más según sus búsquedas
o qué debe incluir una lista de correo para lograr una tasa de respuesta
mayor.</p>
<h3 id="comercio-minorista">Comercio minorista</h3>
<p>Los supermercados emplean los patrones de compra conjunta para
identificar asociaciones de productos y decidir cómo situarlos en los
diferentes pasillos y estanterías en los locales.</p>
<p><i>Data mining</i> permite detectar qué ofertas son las más valoradas
por los clientes y ayuda a definir estrategias para incrementar las
ventas.</p>
<h3 id="banca">Banca</h3>
<p>Los bancos recurren a la minería de datos para entender mejor los
riesgos del mercado.</p>
<p>Es habitual que se aplique a la clalificación crediticia
(<i>reting</i>) y a sistemas inteligentes antifraude para analizar
transacciones, movimientos de tarjetas, patrones de compra y datos
financieros de los clientes.</p>
<h3 id="medicina">Medicina</h3>
<p>La minería de datos favorece diagnósticos más precisos.</p>
<p>Al contar con toda la información del paciente se pueden prescribir
tratamientos más efectivos.</p>
<p>También posibilita una gestión más eficaz, eficiente y económica de
los recursos sanitarios al identificar riesgos, predecir enfermedades en
ciertos segmentos de la población o pronosticar la duración del ingreso
hospitalario.</p>
<h3 id="medios-de-comunicación">Medios de comunicación</h3>
<p>Algunos medios de comunicación aplican la minería de datos en tiempo
real a sus registros de audiencia en televisión online y radio.</p>
<p>Estos sistemas recaban y analizan sobre la marcha información anónima
de las visualizaciones, las retransmisiones y la programación de los
canales para emitir recomendaciones personalizadas a los radioyentes y
telespectadores, conocer en directo sus intereses y su actividad, y
entender mejor su conducta.</p>
<p>Las cadenas obtienen, además, conocimiento muy valioso para sus
anunciantes, que aprovechan estos datos para llegar con más precisión a
sus clientes potenciales.</p>
<h2 id="ii.-técnicas-data-mining">ii. Técnicas data mining</h2>
<h3 id="asociación">Asociación</h3>
<p>La técnica de <strong><u>asociación</u></strong> consiste en
encontrar una correlación o asociación entre datos dentro de un conjunto
extenso de información.</p>
<p>Con esta técnica se pueden descubrir relaciones únicas entre los
datos dentro de un almacén de datos, expresándose dichas relaciones
entre elementos como <u>reglas de asociación</u>.</p>
<p>Este tipo de técnica de minería de datos se utiliza principalmente
para determinar estrategias de marketing y también para aspectos
relacionados con la gestión.</p>
<p>Las reglas de asociación son declaraciones del tipo “<i>if-then</i>”
que ayudan a mostrar la probabilidad de las relaciones dentro de grandes
conjuntos de datos.</p>
<p>La fortaleza de una determinada <u>regla de asociación</u> se mide
mediante dos parámetros principales:</p>
<ul>
<li><p><i>Support:</i> proporción de transacciones que contienen un
determinado conjunto de elementos.</p></li>
<li><p><i>Confidence:</i> se refiere a la cantidad de veces que una
determinada regla resulta ser cierta en la práctica (su solidez o
confiabilidad). En otras palabras, <i>Confidence</i> es el porcentaje de
veces que los elementos de antecedente (la parte “si” de la regla)
aparecen en la misma transacción que los elementos del consecuente (la
parte “entonces” de la regla). Si el valor de <i>confidence</i> es alto,
es indicativo de que la regla es aplicable al conjunto de datos, por lo
que se debe investigar más a fondo dicha regla. La fórmula de
<i>confidence</i> es la siguiente:</p></li>
</ul>
<p><img src="/img/2425/tag3/Tema5_9.png" /></p>
<h3 id="clasificación">Clasificación</h3>
<p>La técnica de <strong>clasificación</strong> tiene su origen en el
<i>machine learning</i>.</p>
<p>Permite clasificar elementos de un conjunto de datos en grupos o
clases <u>predefinidos</u>.</p>
<p>El objetivo de la clasificación es, por lo tanto, predecir con
precisión la clase objetivo para cada caso que aparezca en los
datos.</p>
<p>Una tarea de clasificación comienza con un conjunto de datos en el
que se conocen las asignaciones de clase (clasificación
supervisada).</p>
<ul>
<li><p><u>Objetivo:</u> es la variable que el modelo busca predecir o
estimar.</p></li>
<li><p><u>Predictores:</u> son las variables o características que se
utilizan como entrada para el modelo y que tienen una relación con el
objetivo.</p></li>
<li><p><u>Caso:</u> Es una unidad de observación o instancia que
contiene todos los datos relacionados con una entidad específica en el
análisis.</p></li>
</ul>
<p>El tipo más simple de problema de clasificación es la
<u>clasificación binaria</u>.</p>
<p>En la clasificación binaria, el atributo objetivo tiene solo dos
valores posibles.</p>
<p>Los <u>objetivos multiclase</u> tienen más de dos valores.</p>
<p>La aplicación de un modelo de clasificación da como resultado
<u>asignaciones de clase y probabilidad</u> para cada caso.</p>
<p>La clasificación tiene muchas aplicaciones en la segmentación de
clientes, el modelado de negocios, el marketing, el análisis creditivo y
el modelado biomédico y de respuesta a medicamentos.</p>
<p>Un <u>modelo de clasificación</u> se prueba aplicándolo a datos de
prueba con valores objetivo conocidos y comparando los valores
pronosticados con los valores conocidos.</p>
<p>Las métricas de prueba se utilizan para evaluar la precisión con la
que el modelo predice los valores conocidos.</p>
<p>Si el modelo funciona bien y cumple con los requisitos comerciales,
se puede aplicar a nuevos datos para predecir el futuro.</p>
<p>Una <u>matriz de confusión</u> muestra el número de predicciones
correctas e incorrectas realizadas por el modelo en comparación con las
clasificaciones reales en los datos de la prueba.</p>
<p>La matriz es N por N, donde N es el número de clases.</p>
<p>Los títulos de las filas representan el número de clasificaciones
reales en los datos de la prueba, y los títulos de las columnas
presentan el número de clasificaciones predichas realizadas por el
modelo.</p>
<h3 id="clustering">Clustering</h3>
<p>La técnica de <strong>clustering</strong> (agrupación) encuentra
grupos de objetos de datos que son similares entre sí.</p>
<p>Los miembros de un grupo se parecen más entre sí que como miembros de
otros grupos (aunque distintos clústeres pueden tener miembros en
común).</p>
<p>El objetivo de este tipo de análisis es encontrar agrupaciones de
alta calidad de modo que la similitud entre clústeres sea baja y la
similitud entre miembros de los clústeres sea alta.</p>
<p>La agrupación, como la clasificación, se utiliza para segmentar los
datos.</p>
<p>Sin embargo, a diferencia de la clasificación, los modelos de
agrupamiento segmentan los datos en grupos que <u>no se definieron
previamente</u>.</p>
<p>Los algoritmos de agrupación en clústeres están indicados para
encontrar agrupaciones naturales cuando hay <u>muchos casos y ninguna
agrupación obvia</u>.</p>
<p>El <i>clustering</i> se puede utilizar igualmente para la
<u>detección de anomalías</u>.</p>
<p>Una vez que ha segmentado los datos en grupos, puede determinar que
algunos casos no encajan en ningún tipo.</p>
<p>Estos casos son anomalías o valores atípicos llamados
<i><strong>outliers</strong></i>.</p>
<p>Existen varios enfoques para el cálculo del agrupamiento:</p>
<h4 id="basado-en-la-densidad">Basado en la densidad</h4>
<p>El <i>Density Based Clustering Analisys</i> (DBScan) se basa en la
detección de en qué áreas existen concentraciones de puntos y dónde
están separados por áreas vacías o con escasos puntos.</p>
<p>Las áreas de alta densidad se interpretan como clústeres.</p>
<h5 id="demostración">Demostración</h5>
<p>El punto A y los otros puntos rojos son puntos centrales, porque el
área que rodea estos puntos en un radio ε (umbral de densidad) contiene
al menos 4 puntos (incluido el punto mismo).</p>
<p>Debido a que todos son accesibles entre sí, forman un solo grupo.</p>
<p>Los puntos B y C no son puntos centrales, pero son accesibles desde A
(a través de otros puntos centrales) y, por lo tanto, también pertenecen
al clúster.</p>
<p>El punto N es un punto de ruido que no es accesible.</p>
<p><img src="/img/2425/tag3/Tema5_10.png" /></p>
<h4 id="basado-en-la-distancia">Basado en la distancia</h4>
<p>Este tipo de agrupación utiliza la distancia para determinar la
similitud entre los objetos de datos.</p>
<p>La métrica mide la distancia entre los casos reales y el caso
prototípico del clúster (el que se fija como caso típico).</p>
<p>El caso prototípico se conoce como <strong>centroide</strong>.</p>
<p>El método más conocido de esta técnica es el <i><strong>k-means
clustering</strong></i>, que consiste en agrupar objetos en <i>k</i>
grupos de manera que cada objeto pertenece al grupo cuyo valor medio de
distancias de todos los objetos al centroide le es más cercano (el
agrupamiento se realiza minimizando la suma de distancias entre cada
objeto y el centroide de su clúster).</p>
<p>El algoritmo <i>k-means</i> es del tipo
<u><strong>NP-Hard</strong></u>, aunque existen heurísticas eficientes
que proporcionan agrupaciones próximas al resultado óptimo.</p>
<p>Los centroides pueden ser asignados de inicio o tomados de manera
aleatoria aunque, en este caso, se recalculan los centroides de cada
clúster como la media de todos los puntos del grupo en cada
iteración.</p>
<h4 id="basado-en-cuadrícula">Basado en cuadrícula</h4>
<p>La agrupación en clústeres en cuadrículas se usa generalmente para
conjuntos de datos multidimensionales.</p>
<p>La idea básica de esta técnica es organizar el conjunto de datos con
una representación de cuadrícula y fusionar las celdas de la cuadrícula
para generar grupos.</p>
<p>En esta técnica, el agrupamiento no se basa en los puntos de datos
entre sí, sino en el espacio que rodea a dichos puntos de datos.</p>
<p>El <i>clustering</i> basado en cuadrículas es un algoritmo eficaz
para analizar grandes datos multidimensionales, ya que reduce el tiempo
necesario para buscar los vecinos más cercanos.</p>
<p>Un algoritmo de <i>clustering</i> basado en cuadrículas se llama
STING (STatistical INformation Grid) en el que el área espacial se
divide en celdas rectangulares y varios niveles de celdas con diferentes
niveles de resolución.</p>
<p>Otro algoritmo de <i>clustering</i> basado en cuadrícula es
CLIQUE.</p>
<h5 id="demostración-1">Demostración</h5>
<p>Los algoritmos de <i>clustering</i> basado en cuadrícula precisan
definir el número de cuadrículas y el umbral de densidad.</p>
<p>Si el número de cuadros es demasiado pequeño, se puede dar el casod e
que se sitúen elementos de diferentes grupos en una misma
cuadrícula.</p>
<p>Sin embargo, si el número es demasiado elevado, puede darse el caso
de que haya cuadrículas vacías dentro de los grupos (además de que se
aumenta la complejidad computacional).</p>
<h3 id="regresión">Regresión</h3>
<p>La regresión es una técnica de minería de datos que predice valores
numéricos a lo largo de un periodo continuo.</p>
<p>Las ganancias, las ventas, las tasas hipotecarias, los precios de la
vivienda se pueden predecir utilizando técnicas de regresión.</p>
<p>Una tarea de regresión comienza con un conjunto de datos en el que se
conocen los valores objetivo, aunque estos son independientes entre sí y
aleatorios.</p>
<ul>
<li><p><u>Objetivo:</u> es la variable que el modelo busca predecir o
estimar.</p></li>
<li><p><u>Predictores:</u> son las variables o características que se
utilizan como entrada para el modelo y que tienen una relación con el
objetivo.</p></li>
<li><p><u>Caso:</u> Es una unidad de observación o instancia que
contiene todos los datos relacionados con una entidad específica en el
análisis.</p></li>
</ul>
<p>El modelado de regresión tiene muchas aplicaciones en el análisis de
tendencias, la planificación comercial, el marketing, la previsión
financiera, etc.</p>
<p>Uno de los tipos de regresión más usados es la técnica de
<u>regresión lineal</u>, la cual consiste en encontrar la mejor función
de la recta que permita predecir el valor de una variable sabiendo los
valores de otra variable observada.</p>
<h3 id="series-temporales">Series temporales</h3>
<p>La serie temporal es una técnica de minería de datos que pronostica
el valor objetivo basándose únicamente en un historial conocido,
ordenado y dependiente en el tiempo de valores objetivo.</p>
<p>Es una forma especializada de regresión, conocido como modelado
autorregresivo.</p>
<p>Al igual que otros modelos de regresión, los modelos de serie
temporal aplican métodos matemáticos (ARIMA - <i>AutoRegresive Integrate
Moving Average</i>) <u>que miden la bondad</u> del ajuste de los datos
históricos.</p>
<p>La previsión es un componente crítico de la toma de decisiones
empresariales, por lo que esta técnica tiene aplicaciones a nivel
estratégico, táctico y operativo.</p>
<ul>
<li><p>Proyectar el retorno de inversión (RoI), incluido el crecimiento
y el efecto estratégico de las innovaciones a aplicar.</p></li>
<li><p>Abordar cuestiones tácticas como la proyección de costes, los
requisitos de inventario y la satisfacción del cliente.</p></li>
<li><p>Establecer objetivos operativos y predecir la cantidad y el
cumplimiento de los estándares.</p></li>
</ul>
<p>El algoritmo que utiliza Power BI para mostrar la previsión
(<i>forecast</i>) en un gráfico (algoritmo <i>exponential smoothing</i>)
está basado en este tipo de técnica de <i>data mining</i>.</p>
<h1 id="big-data-en-análisis-empresarial">4. BIG DATA EN ANÁLISIS
EMPRESARIAL</h1>
<h2 id="i.-conceptos-básicos-de-big-data">i. Conceptos básicos de Big
Data</h2>
<p>El término <i>Big Data</i> está referido a la gestión y análisis de
conjuntos de datos cuyo tamaño (volumen), complejidad (variabilidad) y
velocidad de crecimiento (velocidad) dificultan su captura, gestión,
procesamiento o análisis mediante tecnologías y herramientas
tradicionales, tales como bases de datos relacionales y estadísticas
convencionales o paquetes de visualización, <u>dentro del tiempo
necesario</u> para que sean útiles.</p>
<p>La complejidad del <i>Big Data</i> se debe principalmente a la
naturaleza <u>no estructurada</u> (o estructurada de manera heterogénea)
de gran parte de los datos generados por las tecnologías actuales.</p>
<p>Una forma útil de caracterizar el Big Data es con sus tres
dimensiones (las tres V’s): <u>volumen</u>, <u>variedad</u> y
<u>velocidad</u>.</p>
<p>Si bien estas dimensiones engloban los principales atributos, existen
una cuarta y quinta dimensión que han sido añadidas tras consenso por la
comunidad tecnológica que rodea al <i>bog Data</i> y al <i>Business
Intelligence</i>: la <u>veracidad</u> y el <u>valor</u>.</p>
<ul>
<li><p><u><strong>Volumen:</strong></u> el volumen hace referencia a las
cantidades masivas de datos que las organizaciones intentan explotar
para mejorar la toma de decisiones.</p></li>
<li><p><u><strong>Variedad:</strong></u> la variedad tiene que ver con
gestionar la complejidad de múltiples tipos de datos, datos
estructurados, semiestructurados y no estructurados, que se encuentran
tanto de dentro como de fuera de la organización y que están almacenados
en diferentes formatos.</p></li>
<li><p><u><strong>Velocidad:</strong></u> los datos en movimiento. Hoy
en día, los datos se generan de forma exponencial a almacenarlos y
analizarlos. Para los procesos en los que el tiempo resulta fundamental
ciertos tipos de datos deben analizarse en tiempo real para que resulten
útiles para el negocio.</p></li>
<li><p><u><strong>Veracidad:</strong></u> la incertidumbre de los datos.
La veracidad hace referencia al nivel de fiabilidad asociado a ciertos
tipos de datos. Esforzarse por conseguir unos datos de alta calidad es
un requisito importante y un reto fundamental de <i>Big
Data</i>.</p></li>
<li><p><u><strong>Valor:</strong></u> el procesamiento de los datos debe
crear el valor para la organización que garantice el retorno de la
inversión (RoI) al aplicar técnicas <i>Big Data</i> a los
datos.</p></li>
</ul>
<p>El análisis de <i>Big Data</i> ayuda a las organizaciones a
aprovechar sus datos y utilizarlos para identificar nuevas
oportunidades.</p>
<p>Esto conduce a decisiones de negocio más inteligentes, operaciones
más eficientes y mayores ganancias.</p>
<p>Las empresas pueden obtener valor añadido de las siguientes
formas:</p>
<ul>
<li><p><u>Reducción de costes:</u> las grandes tecnologías de datos,
como <i>Hadoop</i> y el análisis basado en la nuebe, aportan importantes
ventajas en términos de costes cuando se trata de almacenar grandes
cantidades de datos, además de identificar maneras más eficientes de
hacer negocios.</p></li>
<li><p><u>Mejor toma de decisiones:</u> con la velocidad de
<i>Hadoop</i> y la analítica en memoria, combinada con la capacidad de
analizar nuevas fuentes de datos, las empresas pueden analizar la
información inmediatamente y tomar decisiones basadas en lo que han
aprendido.</p></li>
<li><p><u>Nuevos productos y servicios:</u> con la capacidad de medir
las necesidades de los clientes y la satisfacción a través de análisis,
las organizaciones pueden orientar sus productos a las necesidades de
consumo de los clientes. Les permite también realizar campañas de
marketing y publicidad más efectivas, procesos comerciales mejorados y
una planificación estratégica más sólida generando mayores ingresos para
el negocio.</p></li>
</ul>
<h2 id="ii.-modelo-mapreduce">ii. Modelo MapReduce</h2>
<p><i>MapReduce</i> es un modelo de programación diseñado para dar
soporte a la computación paralela sobre grandes conjuntos de datos
repartidos entre varios ordenadores.</p>
<p>El nombre está inspirado en los nombres de dos importantes funciones
de programación funcional: <u><i>Map</i></u> y <u><i>Reduce</i></u>.</p>
<p><i>MapReduce</i> ha sido adoptado mundialmente como una
implementación <i>open source</i> denominada <i>Hadoop</i>.</p>
<p>Las funciones <i>Map</i> y <i>Reduce</i> están definidas ambas con
respecto a datos estructurados en tuplas de tipo (clave, valor):</p>
<ul>
<li><p><strong>Map():</strong> <i>Map</i> toma uno de estos pares de
datos con un tipo en un dominio de datos, y devuelve una lista de pares
en un dominio diferente: Map(k1, v1) -&gt; list(k2, v2). <i>Map</i> se
encarga del mapeado, y es aplicada en paralelo para cada elemento de la
entrada produciendo una lista de pares (k2, v2) por cada
llamada.</p></li>
<li><p><strong>Reduce():</strong> la función <i>Reduce</i> es aplicada
en paralelo para cada grupo, produciendo una colección de pares de
valores para cada dominio: Reduce(k2, list(v2)) -&gt; list(k3, v3). Cada
llamada a <i>Reduce</i> gestiona la agregación de los valores producidos
por todos los <i>mappers</i> del sistema (o por la fase <i>shuffle</i>)
de tipo clave-valor en función de su clave. Por último, <i>reducer</i>
genera su fichero de salida de forma independiente.</p></li>
</ul>
<p><i>MapReduce</i>, por lo tanto, transforma una lista de pares (clave,
valor) en una lista de valores.</p>
<ol type="1">
<li><p>Un <i>worker</i> con una tarea <i>map()</i> asignada tiene como
entrada su partición correspondiente. Se dedicará a parsear los datos de
entrada para crear pares de salida (clave, valor). Los pares clave y
valor producidos por la función <i>map()</i> se almacenan como buffer en
la memoria.</p></li>
<li><p>La información escrita en local por los nodos <i>worker</i> de
tipo <i>map</i> es agregada y ordenada por una función agregadora
(shuffer) encargada de realizar esta operación. De esta forma la función
<i>reduce()</i> recibe una lista de valores asociados a una única clave
procedente del combinador.</p></li>
<li><p>El <i>worker</i> de tipo <i>reduce()</i> itera sobre el conjunto
de valores ordenados intermedios, y lo hace por cada una de las claves
únicas encontradas. Toma la clave y el conjunto de valores asociados a
ella y se los pasa a la función <i>reduce()</i>. La salida de
<i>reduce()</i> se añade el archivo de salida de MapReduce.</p></li>
</ol>
<h2 id="iii.-hadoop">iii. Hadoop</h2>
<p>Hadoop es un <i>framework</i> de código abierto con el que se pueden
almacenar y procesar datos de forma masiva.</p>
<p>Tiene la capacidad de operar tareas de forma casi ilimitada con un
gran poder de procesamiento y obtener respuestas rápidas a cualquier
tipo de consulta sobre los datos almacenados.</p>
<p>Esto se consigue gracias a la <u>ejecución distribuida</u> de código
de múltiples nodos, donde cada nodo se encarga de procesar una parte del
trabajo a realizar.</p>
<p><i>Hadoop</i> está inspirado en el proyecto de <i>Google File
System</i> (GFS) y en el paradigma <i>MapReduce</i>.</p>
<p><i>Hadoop</i> está compuesto de cuatro piezas: <i>Hadoop Distributed
File System</i> (HDFS), <i>Hadoop MapReduce</i>, <i>Hadoop YARN</i> y
<i>Hadoop Common</i>.</p>
<p>La arquitectura Hadoop está basada en cuatro elementos
principales:</p>
<ul>
<li><p><u>Hadoop Common:</u> contiene utilidades para todos los módulos,
así como scripts necesarios para iniciar Hadoop.</p></li>
<li><p><u>Hadoop Distributed File System (HDFS):</u> es el sistema de
archivos distribuido que proporciona acceso de alto rendimiento de
datos. Es altamente tolerante a fallos y diseñado para utilizarse en
hardware de bajo coste.</p></li>
<li><p><u>Hadoop YARN <i>(Yet Another Resource Negotiator)</i>:</u>
<i>framework</i> que permite a Hadoop soportar varios motores de
ejecución incluyendo <i>MapReduce</i>, y proporciona un planificador
para los trabajos que se encuentran en ejecución en el clúster. Esta
mejora de <i>Hadoop</i> también es concocida como <i>Hadoop
2</i>.</p></li>
<li><p><u>Hadoop MapReduce:</u> fue creado para hacer frente a problemas
de manera distribuida y su objetivo es el procesamiento en paralelo de
grandes conjuntos de datos.</p></li>
</ul>
<p>Sin embargo, también podemos encontrar:</p>
<ul>
<li><p><strong>Hive:</strong> infraestructura de almacenamiento de
datos.</p></li>
<li><p><strong>Mahout:</strong> implementaciones de algoritmos de
aprendizaje automático centrados principalmente en el álgebra
lineal.</p></li>
<li><p><strong>Lucene:</strong> API de código abierto para recuperación
de información.</p></li>
<li><p><strong>Piges:</strong> plataforma para crear programas. El
lenguaje de esta plataforma es Pig Latin.</p></li>
<li><p><strong>Solr:</strong> motor de búsqueda de código abierto basado
en Lucene.</p></li>
<li><p><strong>Ambari:</strong> gestión de clústeres de Hadoop mediante
interfaz web.</p></li>
<li><p><strong>ZooKeeper:</strong> coordinación de procesos distribuidos
altamente confiables.</p></li>
<li><p><strong>Hbase:</strong> SGBD distribuida no relacional de código
abierto.</p></li>
<li><p><strong>Oozie:</strong> herramienta para definir y ejecutar
flujos de trabajo.</p></li>
</ul>
<p>¿Por qué es importante Hadoop?</p>
<ul>
<li><p><u>Capacidad de almacenar y procesar enormes cantidades de
cualquier tipo de datos</u>.</p></li>
<li><p><u>Poder de cómputo:</u> el modelo de cómputo distribuido de
Hadoop procesa datos a gran velocidad.</p></li>
<li><p><u>Tolerancia a fallos:</u> el procesamiento de datos y
aplicaciones está protegido contra fallos de hardware. Si falla un nodo,
los trabajos son redirigidos automáticamente a otros nodos para
asegurarse de que no falle el procesamiento distribuido. Se almacenan
múltiples copias de todos los datos de manera automática.</p></li>
<li><p><u>Flexibilidad:</u> a diferencia de las bases de datos
relacionales, no tiene que procesar previamente los datos antes de
almacenarlos (puede almacenar tanto datos como desee y decidir cómo
utilizarlos más tarde). Eso incluye datos no estructurados como texto,
imágenes y videos.</p></li>
<li><p><u>Bajo coste:</u> la estructura de código abierto es gratuita y
emplea hardware comercial para almacenar grandes cantidades de
datos.</p></li>
<li><p><u>Escalabilidad:</u> puede hacer crecer fácilmente su sistema
para que procese más datos con solo agregar nodos. Se requiere poca
administración.</p></li>
</ul>
</body>
</html>
